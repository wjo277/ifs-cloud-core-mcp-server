"""
Production-Safe Comprehensive Embedding Framework
=================================================

This framework creates embeddings while respecting copyright laws by:
1. Storing only metadata and references, not source code
2. Using AI-generated summaries (derived work)
3. Allowing users to provide their own code copies at runtime
4. Maintaining full search functionality without copyright issues

Author: AI Assistant
Date: August 17, 2025
"""

import json
import logging
import asyncio
import hashlib
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
import pickle

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm.asyncio import tqdm

from ..plsql_analyzer import ConservativePLSQLAnalyzer
from .ai_summarizer import AISummarizer
from .production_data_structures import (
    ProductionCodeChunk,
    CodeContentLoader,
    convert_to_production_chunk,
)


@dataclass
class ProductionEmbeddingConfig:
    """Configuration for production-safe embedding creation"""

    # Model settings
    model_name: str = "microsoft/unixcoder-base"
    max_tokens: int = 510
    batch_size: int = 8

    # AI enhancement settings (for creating derived work summaries)
    enable_ai_summaries: bool = True
    ai_batch_size: int = 5
    ai_model: str = "qwen3:8b"

    # Processing settings
    max_workers: int = 4

    # Checkpointing settings
    checkpoint_frequency: int = 100
    backup_frequency: int = 500

    # Output settings (metadata only)
    output_dir: Path = Path("production_embeddings")
    cache_dir: Path = Path("production_cache")

    # Copyright compliance
    copyright_owner: str = "IFS"
    include_file_hashes: bool = True  # For integrity verification
    workspace_root: Path = None  # Set during initialization

    # Quality settings
    validate_embeddings: bool = True
    min_summary_length: int = 50  # Minimum AI summary length


@dataclass
class ProductionProcessingStats:
    """Statistics for production embedding creation"""

    total_files: int = 0
    total_chunks: int = 0
    processed_chunks: int = 0
    failed_chunks: int = 0
    ai_enhanced_chunks: int = 0
    chunks_without_content: int = 0  # Chunks that became reference-only
    start_time: datetime = None
    last_checkpoint: datetime = None
    estimated_completion: datetime = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            **asdict(self),
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "last_checkpoint": (
                self.last_checkpoint.isoformat() if self.last_checkpoint else None
            ),
            "estimated_completion": (
                self.estimated_completion.isoformat()
                if self.estimated_completion
                else None
            ),
        }


class ProductionEmbeddingFramework:
    """
    Production-safe embedding framework that respects copyright laws.

    Key Features:
    - Stores only metadata and AI-generated summaries
    - Creates embeddings from derived work (AI summaries), not source code
    - Provides runtime content loading from user's files
    - Full compliance with copyright restrictions
    """

    def __init__(self, config: ProductionEmbeddingConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)

        # Initialize components
        self.ai_summarizer = AISummarizer() if config.enable_ai_summaries else None
        self.plsql_analyzer = ConservativePLSQLAnalyzer()

        # Model components (loaded lazily)
        self.tokenizer = None
        self.model = None
        self.device = None

        # Processing state (production-safe)
        self.stats = ProductionProcessingStats()
        self.processed_files: Set[str] = set()
        self.production_chunks: List[ProductionCodeChunk] = []
        self.embeddings: Optional[np.ndarray] = None

        # Create output directories
        config.output_dir.mkdir(parents=True, exist_ok=True)
        config.cache_dir.mkdir(parents=True, exist_ok=True)

    def _load_model(self):
        """Load the embedding model"""
        if self.tokenizer is not None:
            return

        self.logger.info(f"ü§ñ Loading embedding model: {self.config.model_name}")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            self.model = AutoModel.from_pretrained(self.config.model_name)

            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(self.device)
            self.model.eval()

            self.logger.info(f"‚úÖ Model loaded on device: {self.device}")

        except Exception as e:
            self.logger.error(f"‚ùå Failed to load model: {e}")
            raise

    def _compute_file_hash(self, file_path: Path) -> str:
        """Compute hash of file for integrity checking"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
            return hashlib.md5(content.encode()).hexdigest()
        except Exception:
            return hashlib.md5(str(file_path).encode()).hexdigest()

    def _make_relative_path(self, file_path: Path) -> Path:
        """Convert absolute path to relative path from workspace root"""
        try:
            if self.config.workspace_root:
                return file_path.relative_to(self.config.workspace_root)
            else:
                return file_path
        except ValueError:
            return file_path

    async def _create_production_chunks_for_file(
        self, file_path: Path
    ) -> List[ProductionCodeChunk]:
        """Create production-safe chunks for a file"""
        try:
            # Read file content (temporarily for processing)
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Parse with AST analyzer
            result = self.plsql_analyzer.analyze(content)

            # Calculate file hash for integrity
            file_hash = self._compute_file_hash(file_path)
            relative_path = self._make_relative_path(file_path)
            module_name = self._extract_module_name(file_path)

            production_chunks = []

            for func_info in result.public_methods + result.private_methods:
                # Create production-safe chunk (NO source code stored)
                chunk = ProductionCodeChunk(
                    chunk_id=hashlib.md5(
                        f"{relative_path}:{func_info['line']}".encode()
                    ).hexdigest(),
                    relative_file_path=str(relative_path),
                    file_hash=file_hash,
                    start_line=func_info["line"],
                    end_line=func_info["line"] + 50,  # Estimate end line
                    chunk_type=func_info["type"],  # 'function' or 'procedure'
                    function_name=func_info["name"],
                    language="plsql",
                    module_name=module_name,
                    parsing_method="ast",
                    copyright_owner=self.config.copyright_owner,
                )

                # Extract API calls from surrounding lines (structural analysis, not copyrighted)
                api_calls = []
                try:
                    # Look at a few lines around the function for API calls
                    start_line = max(0, func_info["line"] - 5)
                    end_line = min(len(content.split("\n")), func_info["line"] + 55)
                    surrounding_lines = content.split("\n")[start_line:end_line]
                    surrounding_text = "\n".join(surrounding_lines)

                    api_pattern = r"(\w+_API\.\w+)"
                    api_calls = list(set(re.findall(api_pattern, surrounding_text)))
                except:
                    pass  # If extraction fails, continue without API calls

                chunk.api_calls = api_calls[:10]  # Limit for size

                production_chunks.append(chunk)

            return production_chunks

        except Exception as e:
            self.logger.error(
                f"‚ùå Failed to create production chunks for {file_path}: {e}"
            )
            return []

    def _extract_module_name(self, file_path: Path) -> str:
        """Extract module name from file path"""
        parts = file_path.parts
        if len(parts) >= 2 and parts[-2] != "_work":
            return parts[-2]
        return "unknown"

    async def _enhance_chunks_with_ai(
        self, chunks: List[ProductionCodeChunk], temp_content_map: Dict[str, str]
    ) -> List[ProductionCodeChunk]:
        """
        Enhance chunks with AI-generated summaries.

        This creates derived work (AI summaries) which can be safely embedded
        without copyright issues.
        """
        if not self.ai_summarizer or not self.config.enable_ai_summaries:
            return chunks

        self.logger.info(f"ü§ñ Generating AI summaries for {len(chunks)} chunks...")

        enhanced_chunks = []

        for i in range(0, len(chunks), self.config.ai_batch_size):
            batch = chunks[i : i + self.config.ai_batch_size]

            try:
                # Create temporary chunks with content for AI processing
                temp_chunks = []
                for chunk in batch:
                    temp_content = temp_content_map.get(chunk.chunk_id, "")
                    if temp_content:
                        # Create a temporary CodeChunk-compatible object for AI processing
                        temp_chunk = type(
                            "TempChunk",
                            (),
                            {
                                "chunk_id": chunk.chunk_id,
                                "processed_content": temp_content,
                                "raw_content": temp_content,
                                "function_name": chunk.function_name,
                                "file_path": chunk.relative_file_path,
                                "chunk_type": chunk.chunk_type,
                                "module": chunk.module_name,
                                "business_terms": [],
                                "database_tables": [],
                                "api_calls": chunk.api_calls or [],
                                "layer": None,
                                "start_line": chunk.start_line,
                                "end_line": chunk.end_line,
                            },
                        )()
                        temp_chunks.append(temp_chunk)

                if temp_chunks:
                    # Generate AI summaries
                    summary_results = await self.ai_summarizer.summarize_chunks(
                        temp_chunks, batch_size=self.config.ai_batch_size
                    )

                    # Transfer AI summaries to production chunks (derived work)
                    for j, temp_chunk in enumerate(temp_chunks):
                        if j < len(batch):
                            production_chunk = batch[j]
                            chunk_id = temp_chunk.chunk_id
                            if chunk_id in summary_results:
                                production_chunk.ai_summary = summary_results[chunk_id]
                                production_chunk.ai_purpose = summary_results[
                                    chunk_id
                                ].get("purpose", "")
                                production_chunk.ai_keywords = summary_results[
                                    chunk_id
                                ].get("keywords", None)

                            # Ensure minimum summary quality
                            if (
                                production_chunk.ai_summary
                                and len(production_chunk.ai_summary)
                                >= self.config.min_summary_length
                            ):
                                self.stats.ai_enhanced_chunks += 1

                enhanced_chunks.extend(batch)

            except Exception as e:
                self.logger.warning(
                    f"‚ö†Ô∏è AI enhancement failed for batch {i//self.config.ai_batch_size + 1}: {e}"
                )
                enhanced_chunks.extend(batch)

        return enhanced_chunks

    def _create_embeddings_from_summaries(
        self, chunks: List[ProductionCodeChunk]
    ) -> np.ndarray:
        """
        Create embeddings from AI-generated summaries and metadata only.

        This is copyright-safe as we're embedding derived work (AI summaries)
        and structural metadata, not the original source code.
        """
        if not chunks:
            return np.array([])

        # Prepare texts for embedding (copyright-safe content only)
        searchable_texts = []
        for chunk in chunks:
            # Use only copyright-safe content for embedding
            searchable_text = chunk.get_searchable_text()

            if not searchable_text.strip():
                # Fallback to basic metadata if no AI summary
                basic_text = f"FUNCTION: {chunk.function_name or 'unknown'} MODULE: {chunk.module_name or 'unknown'} TYPE: {chunk.chunk_type}"
                searchable_texts.append(basic_text)
            else:
                searchable_texts.append(searchable_text)

        # Tokenize (batch processing)
        batch_size = self.config.batch_size
        all_embeddings = []

        for i in range(0, len(searchable_texts), batch_size):
            batch_texts = searchable_texts[i : i + batch_size]

            inputs = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=self.config.max_tokens,
                return_tensors="pt",
            )

            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                all_embeddings.append(batch_embeddings)

        if all_embeddings:
            return np.vstack(all_embeddings)
        else:
            return np.array([])

    async def create_production_safe_embeddings(
        self, source_directories: List[Path], workspace_root: Path = None
    ) -> Dict[str, Any]:
        """
        Create production-safe embeddings that respect copyright laws.

        Process:
        1. Extract structural metadata and function signatures
        2. Generate AI summaries (derived work)
        3. Create embeddings from AI summaries + metadata only
        4. Store references to original files, not content
        """

        self.logger.info("üöÄ Creating production-safe embeddings...")

        # Set workspace root for relative paths
        if workspace_root:
            self.config.workspace_root = workspace_root
        elif source_directories:
            self.config.workspace_root = source_directories[0].parent

        # Load model
        self._load_model()

        # Initialize statistics
        self.stats.start_time = datetime.now()

        # Collect all PL/SQL files
        all_files = []
        for source_dir in source_directories:
            if source_dir.exists():
                # IFS Cloud uses .plsql and .plsvc extensions
                plsql_files = (
                    list(source_dir.rglob("*.plsql"))
                    + list(source_dir.rglob("*.plsvc"))
                    + list(source_dir.rglob("*.pls"))
                    + list(source_dir.rglob("*.sql"))
                )
                all_files.extend(plsql_files)

        self.stats.total_files = len(all_files)
        self.logger.info(f"üìÅ Found {len(all_files)} PL/SQL files to process")

        # Temporary content map for AI processing (will be discarded)
        temp_content_map = {}

        # Process files in batches
        progress_bar = tqdm(total=len(all_files), desc="Processing files", unit="files")

        try:
            for file_path in all_files:
                if str(file_path) in self.processed_files:
                    continue

                # Create production-safe chunks
                file_chunks = await self._create_production_chunks_for_file(file_path)

                if file_chunks:
                    # Temporarily store content for AI processing (will be discarded)
                    with open(file_path, "r", encoding="utf-8") as f:
                        file_content = f.read()

                    for chunk in file_chunks:
                        # Extract chunk content temporarily for AI processing
                        lines = file_content.split("\n")
                        if chunk.end_line <= len(lines):
                            chunk_content = "\n".join(
                                lines[chunk.start_line - 1 : chunk.end_line]
                            )
                            temp_content_map[chunk.chunk_id] = chunk_content

                    self.production_chunks.extend(file_chunks)
                    self.processed_files.add(str(file_path))

                progress_bar.update(1)
                progress_bar.set_postfix(
                    {
                        "chunks": len(self.production_chunks),
                        "processed_files": len(self.processed_files),
                    }
                )

        finally:
            progress_bar.close()

        # Enhance with AI summaries (derived work creation)
        if self.config.enable_ai_summaries and temp_content_map:
            self.logger.info("ü§ñ Creating AI-generated summaries (derived work)...")
            self.production_chunks = await self._enhance_chunks_with_ai(
                self.production_chunks, temp_content_map
            )

        # Clear temporary content map (don't store source code)
        temp_content_map.clear()

        # Create embeddings from AI summaries and metadata only
        self.logger.info("üìê Creating embeddings from AI summaries and metadata...")
        self.embeddings = self._create_embeddings_from_summaries(self.production_chunks)

        # Update statistics
        self.stats.total_chunks = len(self.production_chunks)
        self.stats.processed_chunks = len(self.production_chunks)
        self.stats.chunks_without_content = len(
            [c for c in self.production_chunks if not c.ai_summary]
        )

        # Save production-safe results
        results = await self._save_production_results()

        self.logger.info("‚úÖ Production-safe embedding creation completed!")

        return {
            "total_chunks": len(self.production_chunks),
            "total_files": len(self.processed_files),
            "ai_enhanced_chunks": self.stats.ai_enhanced_chunks,
            "chunks_without_content": self.stats.chunks_without_content,
            "embedding_shape": (
                self.embeddings.shape if self.embeddings is not None else None
            ),
            "copyright_compliant": True,
            "stats": self.stats.to_dict(),
            **results,
        }

    async def _save_production_results(self) -> Dict[str, Any]:
        """Save production-safe results (no source code)"""
        output_files = {}

        try:
            # Save embeddings
            embeddings_path = self.config.output_dir / "embeddings.npy"
            if self.embeddings is not None:
                np.save(embeddings_path, self.embeddings)
                output_files["embeddings"] = str(embeddings_path)

            # Save production chunks (metadata only, no source code)
            chunks_path = self.config.output_dir / "production_chunks.json"
            chunks_data = [chunk.to_dict() for chunk in self.production_chunks]

            with open(chunks_path, "w", encoding="utf-8") as f:
                json.dump(chunks_data, f, indent=2, ensure_ascii=False)
            output_files["chunks"] = str(chunks_path)

            # Save content loader configuration
            loader_config_path = self.config.output_dir / "content_loader_config.json"
            loader_config = {
                "workspace_root": (
                    str(self.config.workspace_root)
                    if self.config.workspace_root
                    else None
                ),
                "copyright_owner": self.config.copyright_owner,
                "created_at": datetime.now().isoformat(),
                "total_chunks": len(self.production_chunks),
                "usage_instructions": {
                    "python_example": """
# Load content at runtime from user's files
from production_data_structures import CodeContentLoader, ProductionCodeChunk

loader = CodeContentLoader(workspace_root=Path("your_workspace"))
chunk = ProductionCodeChunk.from_dict(chunk_data)
content = loader.load_chunk_content(chunk)  # User provides their own files
                    """,
                    "important_note": "This system stores only metadata. Users must provide their own copy of the source files.",
                },
            }

            with open(loader_config_path, "w") as f:
                json.dump(loader_config, f, indent=2)
            output_files["loader_config"] = str(loader_config_path)

            # Save processing statistics
            stats_path = self.config.output_dir / "processing_stats.json"
            with open(stats_path, "w") as f:
                json.dump(self.stats.to_dict(), f, indent=2)
            output_files["stats"] = str(stats_path)

            # Save copyright compliance report
            compliance_path = (
                self.config.output_dir / "copyright_compliance_report.json"
            )
            compliance_report = {
                "compliant": True,
                "summary": "This embedding index stores only metadata and AI-generated summaries, not source code",
                "stored_content_types": [
                    "File references (relative paths)",
                    "Function signatures and names",
                    "AI-generated business summaries (derived work)",
                    "Structural metadata (API calls, module names)",
                    "Line number ranges",
                    "File integrity hashes",
                ],
                "not_stored": [
                    "Original source code content",
                    "Copyrighted implementation details",
                    "Proprietary business logic",
                ],
                "copyright_owner": self.config.copyright_owner,
                "created_at": datetime.now().isoformat(),
                "ai_model_used": (
                    self.config.ai_model if self.config.enable_ai_summaries else None
                ),
            }

            with open(compliance_path, "w") as f:
                json.dump(compliance_report, f, indent=2)
            output_files["compliance_report"] = str(compliance_path)

            self.logger.info(
                f"üíæ Production-safe results saved to: {self.config.output_dir}"
            )
            return output_files

        except Exception as e:
            self.logger.error(f"‚ùå Failed to save results: {e}")
            raise


# Convenience function for production use
async def create_production_safe_embeddings(
    source_directories: List[Path],
    workspace_root: Path,
    config: ProductionEmbeddingConfig = None,
) -> Dict[str, Any]:
    """
    Create production-safe embeddings that comply with copyright laws.

    Args:
        source_directories: Directories containing source files (for analysis only)
        workspace_root: Root path for creating relative file references
        config: Configuration (uses defaults if None)

    Returns:
        Results with embeddings created from AI summaries and metadata only
    """
    if config is None:
        config = ProductionEmbeddingConfig()

    config.workspace_root = workspace_root

    framework = ProductionEmbeddingFramework(config)
    return await framework.create_production_safe_embeddings(
        source_directories, workspace_root
    )


if __name__ == "__main__":
    # Example usage
    async def main():
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

        config = ProductionEmbeddingConfig(
            enable_ai_summaries=True,
            batch_size=4,
            ai_batch_size=3,
            output_dir=Path("production_embeddings"),
            copyright_owner="IFS AB",
        )

        workspace_root = Path(".")
        source_dirs = [Path("_work")]

        results = await create_production_safe_embeddings(
            source_directories=source_dirs, workspace_root=workspace_root, config=config
        )

        print("\nüéâ Production-Safe Embedding Creation Complete!")
        print(f"üìä Total chunks: {results['total_chunks']}")
        print(f"ü§ñ AI-enhanced chunks: {results['ai_enhanced_chunks']}")
        print(f"‚öñÔ∏è Copyright compliant: {results['copyright_compliant']}")
        print(f"üìê Embedding shape: {results['embedding_shape']}")

    # asyncio.run(main())
